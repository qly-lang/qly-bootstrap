- because of esrap parser is bottom up, it's not easy to validate quote-unquote levels correct, postpone validation to semantic analysis pass. In short, following should hold:
  - quote level should always ~>= 0~
  - splice must inside an array
  - if there is also quote and unquote in array, (when calculating quote level after apply splice), quote level after array must be negative. E.g.:
    - ~'(@a)~ ok, quote level after array is ~-1~
    - ~''(,@a)~ ok, quote level after array is ~-2~
    - ~'('@a)~ not ok, quote level after array is ~0~

- For a function foo, why saving both foo -> scope-defined-by-foo and mexp-of-foo -> scope-defined-by-foo in scopes?
  - For foo, you can lookup a function's scope by its qly-symbol (note qly-symbol has its location, therefore, two same named symbol will not conflict)
    - TODO: ensure hash table can actually lookup by a struct key - done, use :test 'equalp'
  - For mexp-of-foo, say you have a var defined in body of foo called bar, what scope is bar in? Textually, qly can find which mexp it is in (start, end), and find the smallest scope that contains this range (start, end). After find the scope, you can then lookup definition of bar, given any occur of bar. The reverse is not indexed and may be not needed. In IDE level it's needed as "Find Occurence" or "Find Usage". We might need to index it in compiler, if type inference require it.

Given a qly-symbol, find it's (lexical) scope
- cache: in a pass, possibly resolve-var, record every var's scope
- no cache: find smallest scope that enclosing this var

Given the qly-symbol, have its scope.
- If it's a def, you know it's type
- If it's a occur, you know it's def

Two thing consider to cache:
- symbol -> scope mapping
  - also make scopes only store mexp -> scope mapping. (Done)
  - use another hash table to keep symbol -> scope mapping
- scope -> lookup def by symbol name (already have)
- def -> occurs mapping

Do we need generic? Runtime dispatch vs Compile time dispatch
Two use cases: any generic or bind by protocol type generic.
Any generic does not limit the arg type. Assume every type can be print, A function
#+BEGIN_SRC
f[print-verbose [x]
  print["------"]
  print[x]]
#+END_SRC
without generic this x infers to type ~any~, and it's dispatched at runtime (dynamically check it's type, and print calls print-string, print-int, etc based on x's type). Purpose of generic is to make this happen at compile time, so there generate different version of print-verbose-string, print-verbose-int, etc. and if user call ~print-verbose[1]~ it will be compiled to ~print-verbose-int[1]~ thus, faster. If generic is using on many types, it increases binary size.
Bind by protocol type generic is generic but need to obey some protocol. For example a hash table, the key must obeys Hash protocol, which has a hash function to hash value of this type to a number. Again, without generic this become a runtime dispatch, with a generic it's doing compile time dispatch whenever it's possible. (It's not possible, for example, if using function to process a collection of different kind of Hash object, this case need run time dispatch anyway)
From above we can see generic is not actual a programmaing language feature but rather than an instructive for compiler to choose between "fast execution (more compile time dispatch)" and "small binary (more run time dispatch)". So as the philosophy of qly, compiler should be smart and customizable to handle these case, instead of introduce a new programming language feature of generic. To be smart, qly compiler should do compile time dispatch (auto specialize definitions when there's no or only small increase to binary size). To be customizable, user can:
- provide options to qly compiler cli, giving quantative standard of whether qly should compile something to a compile time dispatch.
- annotate on specific function to override it to do compile time or runtime dispatch.
In short, default should give a balanced performance and binary size in most use case, in special case where user need additional optimization, they still have the power to do so. To mark a function argument as compile time dispatch:
~f[foo:X:'cd]~
TO mark a function argument as runtime time dispatch:
~f[foo:X:'rd]~

Should operator typed?
Advantage of typed operator:
- uniform semantic error checking
- user defined macros can also be typed, make them look and behave exactly same as builtin operators
- type inferencer will be benefit, for example and[x y] infers x and y to be bool, and user can type less. (Or if there's type inference confliction will lead to compilation error)

Will that too complicated and too overhead for builtin operators?
Take ~v~ as an example:
#+BEGIN_SRC
v[x 3]
v[x]
v[x:int]
v[x:int 3]
#+END_SRC
~v~ have four kinds of format, should define all of them:
#+BEGIN_SRC
v[symbol mexp]
v[symbol]
v[symbol:mexp] # or v[symbol:type]?
v[symbol:mexp mexp] # or v[symbol:type mexp-eval-to-type]?
#+END_SRC

It sounds interesting and promising, however, complicated at beginning, consider in a future qly. If that happens, that's possible going to be full qly defined by a code spec, operator spec describes how operator form and semantic, then a true meta compiler (compare to fake Yet Another Compiler Compiler :), will take this spec and generate a compiler for qly. But some places like ~and~ and ~or~ it benefits the type inferencer, so these ones we give them a type. Also, besides typed, qly compiler program have other implied restrictions which is also in qly doc, such as ~v~ must be of above four forms, and in the fourth form ~mexp-eval-to-type~ and ~type~ forms a semantic restriction.

As the initial version of qly, there's no meta type or thing like meta object protocol. ~t~ returns the symbol that names defined type. This symbol is helpful in functions that take a symbol type as a parameter and in macro writings.

Before protocol type, there does not allow function on different type have same name (except for ones that already use match inside)

Do you need to allocate an activation record for function invocation? Can an IR directly handle this?
Yes LLVM IR can handle this by providing a function obj in the llvm lib. Coolgate IR can do something similar. Allocate an activation record is in step of code generation where backend like llvm or coolgate should handle.

Protocol Type / Parametric Polymorphism question 2
Assume a protocol type array-like
#+BEGIN_SRC
t[array-like
  p[array element
    f[get[array uint]:element]
    f[append[array element]:array]]]
#+END_SRC
Now define a function:
#+BEGIN_SRC
f[append2 [a1:array-like.array e1:array-like.element]:array-like.array
  append[a1 e1]]
#+END_SRC
But, what if
#+BEGIN_SRC
f[append3 [a1 e1 a2 e2]
  append[a1 e1]
  append[a2 e2]]
#+END_SRC
Of course this can be type inferred most of time, but how to express ~append3~ type? Easiest:
#+BEGIN_SRC
f[append3 [a1 e1:array-like[a1 e1] a2 e2:array-like[a2 e2]]:e1]
#+END_SRC
What if
#+BEGIN_SRC
f[append4 [a1 e1 a2 e2]
  append[a1 e1]
  append[a2 e2]
  append[a1 e2]]
#+END_SRC
This infer to:
#+BEGIN_SRC
array-like[a1 e1]
array-like[a2 e2]
array-like[a1 e2]
#+END_SRC
But this does not mean e1 must be same type of e2. Protocol type notation above only specify: with whom a parameter forms a protocol what, but not some type has to or can be not same as another type. In above case, e2 satisfy two protocol type. But any param can only satisfy one protocol type. Because of this, qly will define an implicit protocol type that include three append relation.

Also, qly doesn't have way to restrict two type to be same. For example:
#+BEGIN_SRC
f[foo [x:any y:any]]
#+END_SRC
x simply can be any type, same or not same as y. They can be optimized to specialized no matter x need to be same or not as y. When ~foo~ called with two same type, for example int, it's optimized to a ~f[foo [int int]]~ call, have same efficient result as a parametric polymorphism language. There's no good reason to limit x must be same as y.
