- because of esrap parser is bottom up, it's not easy to validate quote-unquote levels correct, postpone validation to semantic analysis pass. In short, following should hold:
  - quote level should always ~>= 0~
  - splice must inside an array
  - if there is also quote and unquote in array, (when calculating quote level after apply splice), quote level after array must be negative. E.g.:
    - ~'(@a)~ ok, quote level after array is ~-1~
    - ~''(,@a)~ ok, quote level after array is ~-2~
    - ~'('@a)~ not ok, quote level after array is ~0~

- For a function foo, why saving both foo -> scope-defined-by-foo and mexp-of-foo -> scope-defined-by-foo in scopes?
  - For foo, you can lookup a function's scope by its qly-symbol (note qly-symbol has its location, therefore, two same named symbol will not conflict)
    - TODO: ensure hash table can actually lookup by a struct key - done, use :test 'equalp'
  - For mexp-of-foo, say you have a var defined in body of foo called bar, what scope is bar in? Textually, qly can find which mexp it is in (start, end), and find the smallest scope that contains this range (start, end). After find the scope, you can then lookup definition of bar, given any occur of bar. The reverse is not indexed and may be not needed. In IDE level it's needed as "Find Occurence" or "Find Usage". We might need to index it in compiler, if type inference require it.

Given a qly-symbol, find it's (lexical) scope
- cache: in a pass, possibly resolve-var, record every var's scope
- no cache: find smallest scope that enclosing this var

Given the qly-symbol, have its scope.
- If it's a def, you know it's type
- If it's a occur, you know it's def

Two thing consider to cache:
- symbol -> scope mapping
  - also make scopes only store mexp -> scope mapping. (Done)
  - use another hash table to keep symbol -> scope mapping
- scope -> lookup def by symbol name (already have)
- def -> occurs mapping

Do we need generic? Runtime dispatch vs Compile time dispatch
Two use cases: any generic or bind by protocol type generic.
Any generic does not limit the arg type. Assume every type can be print, A function
#+BEGIN_SRC
f[print-verbose [x]
  print["------"]
  print[x]]
#+END_SRC
without generic this x infers to type ~any~, and it's dispatched at runtime (dynamically check it's type, and print calls print-string, print-int, etc based on x's type). Purpose of generic is to make this happen at compile time, so there generate different version of print-verbose-string, print-verbose-int, etc. and if user call ~print-verbose[1]~ it will be compiled to ~print-verbose-int[1]~ thus, faster. If generic is using on many types, it increases binary size.
Bind by protocol type generic is generic but need to obey some protocol. For example a hash table, the key must obeys Hash protocol, which has a hash function to hash value of this type to a number. Again, without generic this become a runtime dispatch, with a generic it's doing compile time dispatch whenever it's possible. (It's not possible, for example, if using function to process a collection of different kind of Hash object, this case need run time dispatch anyway)
From above we can see generic is not actual a programmaing language feature but rather than an instructive for compiler to choose between "fast execution (more compile time dispatch)" and "small binary (more run time dispatch)". So as the philosophy of qly, compiler should be smart and customizable to handle these case, instead of introduce a new programming language feature of generic. To be smart, qly compiler should do compile time dispatch (auto specialize definitions when there's no or only small increase to binary size). To be customizable, user can:
- provide options to qly compiler cli, giving quantative standard of whether qly should compile something to a compile time dispatch.
- annotate on specific function to override it to do compile time or runtime dispatch.
In short, default should give a balanced performance and binary size in most use case, in special case where user need additional optimization, they still have the power to do so. To mark a function argument as compile time dispatch:
~f[foo:X:'cd]~
TO mark a function argument as runtime time dispatch:
~f[foo:X:'rd]~

Should operator typed?
Advantage of typed operator:
- uniform semantic error checking
- user defined macros can also be typed, make them look and behave exactly same as builtin operators
- type inferencer will be benefit, for example and[x y] infers x and y to be bool, and user can type less. (Or if there's type inference confliction will lead to compilation error)

Will that too complicated and too overhead for builtin operators?
Take ~v~ as an example:
#+BEGIN_SRC
v[x 3]
v[x]
v[x:int]
v[x:int 3]
#+END_SRC
~v~ have four kinds of format, should define all of them:
#+BEGIN_SRC
v[symbol mexp]
v[symbol]
v[symbol:mexp] # or v[symbol:type]?
v[symbol:mexp mexp] # or v[symbol:type mexp-eval-to-type]?
#+END_SRC

It sounds interesting and promising, however, complicated at beginning, consider in a future qly. If that happens, that's possible going to be full qly defined by a code spec, operator spec describes how operator form and semantic, then a true meta compiler (compare to fake Yet Another Compiler Compiler :), will take this spec and generate a compiler for qly. But some places like ~and~ and ~or~ it benefits the type inferencer, so these ones we give them a type. Also, besides typed, qly compiler program have other implied restrictions which is also in qly doc, such as ~v~ must be of above four forms, and in the fourth form ~mexp-eval-to-type~ and ~type~ forms a semantic restriction.

As the initial version of qly, there's no meta type or thing like meta object protocol. ~t~ returns the symbol that names defined type. This symbol is helpful in functions that take a symbol type as a parameter and in macro writings.

Before protocol type, there does not allow function on different type have same name (except for ones that already use match inside)

Do you need to allocate an activation record for function invocation? Can an IR directly handle this?
Yes LLVM IR can handle this by providing a function obj in the llvm lib. Coolgate IR can do something similar. Allocate an activation record is in step of code generation where backend like llvm or coolgate should handle.

Protocol Type / Parametric Polymorphism question 2
Assume a protocol type array-like
#+BEGIN_SRC
t[array-like
  p[array element
    f[get[array uint]:element]
    f[append[array element]:array]]]
#+END_SRC
Now define a function:
#+BEGIN_SRC
f[append2 [a1:array-like.array e1:array-like.element]:array-like.array
  append[a1 e1]]
#+END_SRC
But, what if
#+BEGIN_SRC
f[append3 [a1 e1 a2 e2]
  append[a1 e1]
  append[a2 e2]]
#+END_SRC
Of course this can be type inferred most of time, but how to express ~append3~ type? Easiest:
#+BEGIN_SRC
f[append3 [a1 e1:array-like[a1 e1] a2 e2:array-like[a2 e2]]:e1]
#+END_SRC
What if
#+BEGIN_SRC
f[append4 [a1 e1 a2 e2]
  append[a1 e1]
  append[a2 e2]
  append[a1 e2]]
#+END_SRC
This infer to:
#+BEGIN_SRC
array-like[a1 e1]
array-like[a2 e2]
array-like[a1 e2]
#+END_SRC
But this does not mean e1 must be same type of e2. Protocol type notation above only specify: with whom a parameter forms a protocol what, but not some type has to or can be not same as another type. In above case, e2 satisfy two protocol type. But any param can only satisfy one protocol type. Because of this, qly will define an implicit protocol type that include three append relation.

Also, qly doesn't have way to restrict two type to be same. For example:
#+BEGIN_SRC
f[foo [x:any y:any]]
#+END_SRC
x simply can be any type, same or not same as y. They can be optimized to specialized no matter x need to be same or not as y. When ~foo~ called with two same type, for example int, it's optimized to a ~f[foo [int int]]~ call, have same efficient result as a parametric polymorphism language. There's no good reason to limit x must be same as y.

Assume there's no import, every qly is only one file, no namespace now. How to resolve ~a.b~?
If a is a record type, and b is a known field, then ~.~ means field access
If a is not a record type, or a doesn't have record field, then ~.~ means function call ~b[a]~

If a is not a symbol, instead an expression, check whether it return type is a field type, has field type b. This means to resolve a call exp, must already know types.

Do you really need quote unquote?
- In function programming, you can construct array via array literal, no quote is needed.
- In macro programmming, for example, make a ~while~ based on ~loop~ and ~if~:
#+BEGIN_SRC
m[while[condition:mexp body:[mexp]]
  'loop[if[not[,mexp]
            break[]]
         @body]]
#+END_SRC

Without quote exp this become more difficult:
#+BEGIN_SRC
m[while[condition:mexp body:[mexp]]
  call-exp['loop call-exp['if call-exp['not mexp] 'break[]]] body]

#+END_SRC
Not easier

Is or type really good and necessary compare to subtype (subclass)
Example: Exp, includes AExp and BExp.
Use or:
#+BEGIN_SRC
t[Exp or[AExp BExp]]
#+END_SRC
Problem, if adding a C, you must add a CExp by modify Exp type definition

Use subtype:
#+BEGIN_SRC
<:[AExp Exp]
<:[BExp Exp]
#+END_SRC
If adding a C, it can be add without modify existing code. But, consider
#+BEGIN_SRC
t[List or[cons[car:Elem cdr:List] Nil]]
#+END_SRC
Can this be expressed by subtype? Yes
#+BEGIN_SRC
abstract[List]
<:[Nil List]
<:[cons[car:Elem cdr:List] List] # or <:[NonNil List] and t[NonNil cons[...]]
#+END_SRC
Can a type be a type of several types? Yes
Can a concrete type extended by a subtype? No, you can create a new abstract type, make the concrete type as one subtype of the abstract type and make new type (either concrete or abstract) as new subtypes of the new abstract type.
Polymorphism? Easy, define function on abstract types, that can take both abstract and concrete types.
#+BEGIN_SRC
abstract[Printable]
<:[number Printable]
<:[string Printable]

f[print [x:Printable] ...]

f[[x:string]
  ...
  print[x]]
#+END_SRC

Do you need Parametric Types?
Use case:
1. compile time generic type
2. constraint different types to be same or one type is same as one's parameter

#+BEGIN_SRC
f[my-append [x:[any] y:any]:[any]]
my-append[["aa" "bb"] "cc"] # happens to be same, can compile to a my-append-string
my-append[[3 4] 5] # happens to be same, can compile to a my-append-int
my-append[["ab" "c"] 4] # not same, compile to dynamic dispatch version

f[my-append2 [x:[any] y:any x1:[any] y2:any]:[any]]
# Do you have to specify y's any is same as x's and y2's any is same as x1's element, and return type is same as y[2]? Can qly auto figure out that and auto specialize my-append2 in compilation?

#+END_SRC

But unless you can analyze all possible kinds of input, it's not possible to predicate return type above is of same type as y, y2, or neither (really? How does type inferencer work? Type inferencer requires you to annote [any]'s relationship of one return args, there's no way to specify what it's relationship between args type without such annotation)
But, what if you can specify return type is same as which's return type?? Do you still need parametric types then?
#+BEGIN_SRC
f[my-append2 [x:[any] y:any x1:[any] y2:any]:t[x]]
#+END_SRC
Still not, you don't know structure of what my-append2 called, for example it may call my-append. It might be possible if you annotate all function this way, but this also hard and hard to reasoning.

What if adding parametric type?
#+BEGIN_SRC
f[my-append[t] [x:[t] y:t]:[t]]
#+END_SRC
What if t must satisfy some constraints, for example, subtype of Printable,
#+BEGIN_SRC
f[my-append[t:Printable] [x:[t] y:t]:[t]]
#+END_SRC
Of course all these types are most of time type inferred, you write
#+BEGIN_SRC
f[my-append [x y]
  append[x y]]
#+END_SRC
will infer
#+BEGIN_SRC
f[my-append[t] [x:[t] y:t]:[t]]
#+END_SRC
You write
#+BEGIN_SRC
f[my-append [x y]
  print[x]
  append[x y]]
#+END_SRC
will infer
#+BEGIN_SRC
f[my-append[t [t]:printable] [x:[t] y:t]:[t]]
#+END_SRC
etc.

How is printable defined? How?
1. There is ~print[x]~
2. There're many definitions of ~print[x]~,
#+BEGIN_SRC
m[print [x:string]]
m[print [x:int]]
gf[print [x:printable]]
t[printable]
t[int:printable]
t[string:printable]
#+END_SRC
major challenge is how to link an abstract type to an abstract function definition. Above present a solution.
- If you add a new type in printable - you need
#+BEGIN_SRC
t[n:printable]
m[print [x:n]]
#+END_SRC
- If you need add a new operation, say, debug-print, to all printable, you can either create a f, take advantage of gf, or create a gf, customize every type by m.

So since you know gf[print [x:printable]] you immediately knows
#+BEGIN_SRC
f[my-append [x y]
  print[x]
  append[x y]]
#+END_SRC
will infer
#+BEGIN_SRC
f[my-append[t [t]:printable] [x:[t] y:t]:[t]]
#+END_SRC
